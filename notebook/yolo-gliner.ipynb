{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8c617c367c4f401abbda450b29576ebe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b40a6c7255894154b176a8252a7f0e67","IPY_MODEL_8343c5cc20664edda5768f06e31f328b","IPY_MODEL_205edf438fe04ef0bf85a700008ef596"],"layout":"IPY_MODEL_8096e916f5854009a79c21c446c9c2a3"}},"b40a6c7255894154b176a8252a7f0e67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95faddcabd5949c3beff5173af0888df","placeholder":"​","style":"IPY_MODEL_7615386532ae46bc87bdb44353b94fa8","value":"100%"}},"8343c5cc20664edda5768f06e31f328b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66583103c51f424ebd6a149e5eb1d361","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b5c6710f9ad41d9ae913e4192d7bc95","value":20}},"205edf438fe04ef0bf85a700008ef596":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0d07eb7927840a783c77cd0f7f27540","placeholder":"​","style":"IPY_MODEL_93692760100c4612924933d649eeb442","value":" 20/20 [00:44&lt;00:00,  1.89s/it]"}},"8096e916f5854009a79c21c446c9c2a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95faddcabd5949c3beff5173af0888df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7615386532ae46bc87bdb44353b94fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66583103c51f424ebd6a149e5eb1d361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b5c6710f9ad41d9ae913e4192d7bc95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0d07eb7927840a783c77cd0f7f27540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93692760100c4612924933d649eeb442":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f34a8bf789164884bcde9fd9aab95a76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7872c752444d4df486b882015ae67218","IPY_MODEL_2b0ec1e1cde54a388e23d4f89c4d2c00","IPY_MODEL_78b25f4e0c9543aaa6ea683452d2ef41"],"layout":"IPY_MODEL_6a973618df3142adb0d3ce52437d85e0"}},"7872c752444d4df486b882015ae67218":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddb56ffce61442c292fb05ee47ea18d0","placeholder":"​","style":"IPY_MODEL_6c86344bc5254f11bacc112b587512f5","value":"100%"}},"2b0ec1e1cde54a388e23d4f89c4d2c00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db7425f91cbe48c389fd5520954869c3","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e5e0e97a1c44a0da4c162eacff2469c","value":20}},"78b25f4e0c9543aaa6ea683452d2ef41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c1d9348d0424738b118776b8471cd88","placeholder":"​","style":"IPY_MODEL_a7cdb06177504f478e41c83472a6bf6a","value":" 20/20 [00:09&lt;00:00,  2.33it/s]"}},"6a973618df3142adb0d3ce52437d85e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddb56ffce61442c292fb05ee47ea18d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c86344bc5254f11bacc112b587512f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db7425f91cbe48c389fd5520954869c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e5e0e97a1c44a0da4c162eacff2469c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c1d9348d0424738b118776b8471cd88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7cdb06177504f478e41c83472a6bf6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11115148,"sourceType":"datasetVersion","datasetId":6930367},{"sourceId":11115573,"sourceType":"datasetVersion","datasetId":6930681},{"sourceId":122725815,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport ast\nimport shutil as sh\nfrom pathlib import Path\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport PIL\n\nimport torch\n\nfrom tqdm.auto import tqdm\n\nfrom IPython.display import Image, clear_output\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"gDQkoOvTDSE3","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:38:26.190982Z","iopub.execute_input":"2025-03-21T09:38:26.191274Z","iopub.status.idle":"2025-03-21T09:38:30.483925Z","shell.execute_reply.started":"2025-03-21T09:38:26.191247Z","shell.execute_reply":"2025-03-21T09:38:30.483205Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:48:37.058477Z","iopub.execute_input":"2025-03-21T09:48:37.058965Z","iopub.status.idle":"2025-03-21T09:48:37.185623Z","shell.execute_reply.started":"2025-03-21T09:48:37.058929Z","shell.execute_reply":"2025-03-21T09:48:37.184611Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'yolov5' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install -qr /kaggle/working/yolov5/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:49:28.735166Z","iopub.execute_input":"2025-03-21T09:49:28.735505Z","iopub.status.idle":"2025-03-21T09:49:32.117320Z","shell.execute_reply.started":"2025-03-21T09:49:28.735478Z","shell.execute_reply":"2025-03-21T09:49:32.116180Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Convert annotations to dataframe","metadata":{"id":"0IZy-GAK-cFK"}},{"cell_type":"code","source":"import json","metadata":{"id":"09G6uFa3-pBh","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:53:17.200397Z","iopub.execute_input":"2025-03-21T09:53:17.200811Z","iopub.status.idle":"2025-03-21T09:53:17.204931Z","shell.execute_reply.started":"2025-03-21T09:53:17.200779Z","shell.execute_reply":"2025-03-21T09:53:17.204100Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"ANNOTATION_DIR = \"/kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/ann\"","metadata":{"id":"TygDST_OBTr0","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:53:43.221569Z","iopub.execute_input":"2025-03-21T09:53:43.221877Z","iopub.status.idle":"2025-03-21T09:53:43.225505Z","shell.execute_reply.started":"2025-03-21T09:53:43.221853Z","shell.execute_reply":"2025-03-21T09:53:43.224721Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def create_dataframe(img_name, data, df):\n\n  for obj in data['objects']:\n    label = obj['classTitle']\n    bounding_box = obj['points']['exterior']\n    width = bounding_box[1][0] - bounding_box[0][0]\n    height = bounding_box[1][1] - bounding_box[0][1]\n    final_list = []\n    all_bounds = []\n    for bounds in bounding_box:\n      final_list.append(bounds[0])\n      final_list.append(bounds[1])\n    final_list = tuple(final_list)\n    all_bounds.append(final_list)\n    input_data = {\n        \"img_name\":img_name,\n        \"class\":label,\n        \"bounds\":all_bounds,\n        \"width\":width,\n        \"height\":height,\n    }\n\n    tmp_df = pd.DataFrame(input_data)\n    if(df.shape[0] == 0):\n      df = tmp_df.copy()\n    else:\n      df = pd.concat([df, tmp_df], axis=0, ignore_index=True)\n  return df\n\n\nfiles_list = os.listdir(ANNOTATION_DIR)\ndf = pd.DataFrame()\nfor files in files_list:\n  img_name = files.split(\".json\")[0]\n  with open(f\"{ANNOTATION_DIR}/{files}\") as f:\n    data = json.load(f)\n  df = create_dataframe(img_name, data, df)","metadata":{"id":"o13bUFFx_Ga1","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:53:53.745936Z","iopub.execute_input":"2025-03-21T09:53:53.746265Z","iopub.status.idle":"2025-03-21T09:53:53.834203Z","shell.execute_reply.started":"2025-03-21T09:53:53.746244Z","shell.execute_reply":"2025-03-21T09:53:53.833565Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3Fjrt6Zo-6NN","outputId":"ab9c011d-ea35-41e2-a97c-0c214ae38215","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:06.788410Z","iopub.execute_input":"2025-03-21T09:54:06.788700Z","iopub.status.idle":"2025-03-21T09:54:06.798204Z","shell.execute_reply.started":"2025-03-21T09:54:06.788678Z","shell.execute_reply":"2025-03-21T09:54:06.797333Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                               img_name      class                   bounds  \\\n0  Detailed_Divorce_Agreement_2_pg2.jpg  Signature  (609, 1275, 1390, 1665)   \n1  Detailed_Divorce_Agreement_2_pg2.jpg  Signature  (616, 2201, 1377, 2516)   \n2  Detailed_Divorce_Agreement_2_pg2.jpg  Signature  (623, 3100, 1390, 3394)   \n3  Detailed_Divorce_Agreement_2_pg2.jpg  Signature  (602, 3902, 1452, 4251)   \n4  Detailed_Divorce_Agreement_5_pg3.jpg       Logo  (643, 1879, 1431, 2578)   \n\n   width  height  \n0    781     390  \n1    761     315  \n2    767     294  \n3    850     349  \n4    788     699  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_name</th>\n      <th>class</th>\n      <th>bounds</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Detailed_Divorce_Agreement_2_pg2.jpg</td>\n      <td>Signature</td>\n      <td>(609, 1275, 1390, 1665)</td>\n      <td>781</td>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Detailed_Divorce_Agreement_2_pg2.jpg</td>\n      <td>Signature</td>\n      <td>(616, 2201, 1377, 2516)</td>\n      <td>761</td>\n      <td>315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Detailed_Divorce_Agreement_2_pg2.jpg</td>\n      <td>Signature</td>\n      <td>(623, 3100, 1390, 3394)</td>\n      <td>767</td>\n      <td>294</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Detailed_Divorce_Agreement_2_pg2.jpg</td>\n      <td>Signature</td>\n      <td>(602, 3902, 1452, 4251)</td>\n      <td>850</td>\n      <td>349</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Detailed_Divorce_Agreement_5_pg3.jpg</td>\n      <td>Logo</td>\n      <td>(643, 1879, 1431, 2578)</td>\n      <td>788</td>\n      <td>699</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Tile Images","metadata":{"id":"oewedPrFHBew"}},{"cell_type":"code","source":"val_df = df.iloc[45:]\nval_index = val_df['img_name'].unique()\nval_index","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh2OAIHW9k_z","outputId":"8ef1aa80-163b-4b3c-c610-b0c0fead8c30","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:14.150315Z","iopub.execute_input":"2025-03-21T09:54:14.150582Z","iopub.status.idle":"2025-03-21T09:54:14.161730Z","shell.execute_reply.started":"2025-03-21T09:54:14.150562Z","shell.execute_reply":"2025-03-21T09:54:14.161039Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array(['Detailed_Divorce_Agreement_1_pg2.jpg',\n       'Detailed_Divorce_Agreement_1_pg3.jpg'], dtype=object)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import os\nimport tqdm.notebook\nTILE_WIDTH = 1200\nTILE_HEIGHT = 1200\nTILE_OVERLAP = 64\nTRUNCATED_PERCENT = 0.3\n_overwriteFiles = True\n\nTILES_DIR = {'train': Path('train3/images'),\n             'val': Path('val3/images/')}\nfor _, folder in TILES_DIR.items():\n    if not os.path.isdir(folder):\n        os.makedirs(folder)","metadata":{"id":"9ZNxJQOIAQwj","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:19.308193Z","iopub.execute_input":"2025-03-21T09:54:19.308480Z","iopub.status.idle":"2025-03-21T09:54:19.313540Z","shell.execute_reply.started":"2025-03-21T09:54:19.308460Z","shell.execute_reply":"2025-03-21T09:54:19.312797Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Define augmentation pipeline\ndef get_augmentation_pipeline():\n    return A.Compose([\n        A.HorizontalFlip(p=0.1),\n        A.RandomBrightnessContrast(p=0.2),\n        A.Rotate(limit=15, p=0.1),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n        A.Resize(height=TILE_HEIGHT, width=TILE_WIDTH, always_apply=True),\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\n# Initialize augmentation pipeline\naugmentation_pipeline = get_augmentation_pipeline()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Zc5g3npReTt","outputId":"d25a5e90-2d49-492c-981c-01f6e40c3ad0","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:22.380430Z","iopub.execute_input":"2025-03-21T09:54:22.380726Z","iopub.status.idle":"2025-03-21T09:54:24.075150Z","shell.execute_reply.started":"2025-03-21T09:54:22.380705Z","shell.execute_reply":"2025-03-21T09:54:24.074100Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"LABELS_DIR = {'train': Path('train3/labels/'),\n              'val': Path('val3/labels/')}\nfor _, folder in LABELS_DIR.items():\n    if not os.path.isdir(folder):\n        os.makedirs(folder)","metadata":{"id":"32INE5L9AVES","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:26.356892Z","iopub.execute_input":"2025-03-21T09:54:26.357398Z","iopub.status.idle":"2025-03-21T09:54:26.362154Z","shell.execute_reply.started":"2025-03-21T09:54:26.357367Z","shell.execute_reply":"2025-03-21T09:54:26.361232Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"IMG_DIR = \"/kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img\"\nimg_list = os.listdir(IMG_DIR)","metadata":{"id":"IPN8n244AoJR","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:54:45.099571Z","iopub.execute_input":"2025-03-21T09:54:45.099908Z","iopub.status.idle":"2025-03-21T09:54:45.115548Z","shell.execute_reply.started":"2025-03-21T09:54:45.099879Z","shell.execute_reply":"2025-03-21T09:54:45.114882Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"len(img_list)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETDQa2rmBswV","outputId":"b7e2a4dd-118d-42f0-ecd9-443ecdef7d3d","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:55:02.923823Z","iopub.execute_input":"2025-03-21T09:55:02.924179Z","iopub.status.idle":"2025-03-21T09:55:02.929374Z","shell.execute_reply.started":"2025-03-21T09:55:02.924151Z","shell.execute_reply":"2025-03-21T09:55:02.928612Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"IMAGE_WIDTH = 4250\nIMAGE_HEIGHT = 5500","metadata":{"id":"ay_tcc-LBki7","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:55:05.665532Z","iopub.execute_input":"2025-03-21T09:55:05.665813Z","iopub.status.idle":"2025-03-21T09:55:05.669307Z","shell.execute_reply.started":"2025-03-21T09:55:05.665791Z","shell.execute_reply":"2025-03-21T09:55:05.668448Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def tag_is_inside_tile(bounds, x_start, y_start, width, height, truncated_percent):\n    x_min, y_min, x_max, y_max = bounds\n    x_min, y_min, x_max, y_max = x_min - x_start, y_min - y_start, x_max - x_start, y_max - y_start\n\n    if (x_min > width) or (x_max < 0.0) or (y_min > height) or (y_max < 0.0):\n        return None\n\n    x_max_trunc = min(x_max, width)\n    x_min_trunc = max(x_min, 0)\n    if (x_max_trunc - x_min_trunc) / (x_max - x_min) < truncated_percent:\n        return None\n\n    y_max_trunc = min(y_max, width)\n    y_min_trunc = max(y_min, 0)\n    if (y_max_trunc - y_min_trunc) / (y_max - y_min) < truncated_percent:\n        return None\n\n    x_center = (x_min_trunc + x_max_trunc) / 2.0 / width\n    y_center = (y_min_trunc + y_max_trunc) / 2.0 / height\n    x_extend = (x_max_trunc - x_min_trunc) / width\n    y_extend = (y_max_trunc - y_min_trunc) / height\n\n    return (0, x_center, y_center, x_extend, y_extend)\n\nfor img_path in tqdm.notebook.tqdm(img_list):\n    pil_img = PIL.Image.open(f\"{IMG_DIR}/{img_path}\", mode='r')\n    np_img = np.array(pil_img, dtype=np.uint8)\n\n    img_labels = df[df[\"img_name\"] == img_path]\n    X_TILES = (IMAGE_WIDTH + TILE_WIDTH - TILE_OVERLAP - 1) // (TILE_WIDTH - TILE_OVERLAP)\n    Y_TILES = (IMAGE_HEIGHT + TILE_HEIGHT - TILE_OVERLAP - 1) // (TILE_HEIGHT - TILE_OVERLAP)\n\n    for x in range(X_TILES):\n        for y in range(Y_TILES):\n\n            x_end = min((x + 1) * TILE_WIDTH - TILE_OVERLAP * (x != 0), IMAGE_WIDTH)\n            x_start = x_end - TILE_WIDTH\n            y_end = min((y + 1) * TILE_HEIGHT - TILE_OVERLAP * (y != 0), IMAGE_HEIGHT)\n            y_start = y_end - TILE_HEIGHT\n\n            folder = 'val' if img_path in val_index else 'train'\n            save_tile_path = TILES_DIR[folder].joinpath(img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + \".jpg\")\n            save_label_path = LABELS_DIR[folder].joinpath(img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + \".txt\")\n\n            cut_tile = np.zeros(shape=(TILE_WIDTH, TILE_HEIGHT, 3), dtype=np.uint8)\n            cut_tile[0:TILE_HEIGHT, 0:TILE_WIDTH, :] = np_img[y_start:y_end, x_start:x_end, :]\n\n\n            found_tags = [\n                tag_is_inside_tile(bounds, x_start, y_start, TILE_WIDTH, TILE_HEIGHT, TRUNCATED_PERCENT)\n                for i, bounds in enumerate(img_labels['bounds'])]\n            found_tags = [el for el in found_tags if el is not None]\n\n            if len(found_tags) > 0:\n                for dup_index in range(30):  # Duplicate 10 times\n                    duplicated_tile_path = TILES_DIR[folder].joinpath(\n                        img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + f\"_dup{dup_index}.jpg\"\n                    )\n                    duplicated_label_path = LABELS_DIR[folder].joinpath(\n                        img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + f\"_dup{dup_index}.txt\"\n                    )\n\n                    # Save duplicated image\n                    duplicated_tile_img = PIL.Image.fromarray(cut_tile)\n                    duplicated_tile_img.save(duplicated_tile_path)\n\n                    # Save duplicated labels\n                    with open(duplicated_label_path, 'w+') as f:\n                        for tags in found_tags:\n                            f.write(' '.join(str(x) for x in tags) + '\\n')\n\n            else:\n              if _overwriteFiles or not os.path.isfile(save_tile_path):\n                    cut_tile_img = PIL.Image.fromarray(cut_tile)\n                    cut_tile_img.save(save_tile_path)\n            with open(save_label_path, 'w+') as f:\n                for tags in found_tags:\n                    f.write(' '.join(str(x) for x in tags) + '\\n')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["8c617c367c4f401abbda450b29576ebe","b40a6c7255894154b176a8252a7f0e67","8343c5cc20664edda5768f06e31f328b","205edf438fe04ef0bf85a700008ef596","8096e916f5854009a79c21c446c9c2a3","95faddcabd5949c3beff5173af0888df","7615386532ae46bc87bdb44353b94fa8","66583103c51f424ebd6a149e5eb1d361","7b5c6710f9ad41d9ae913e4192d7bc95","a0d07eb7927840a783c77cd0f7f27540","93692760100c4612924933d649eeb442"]},"id":"hW-OXRJWR5n_","outputId":"903af2a4-465b-4262-a74e-21d2b21bc674","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:55:08.808563Z","iopub.execute_input":"2025-03-21T09:55:08.809064Z","iopub.status.idle":"2025-03-21T09:55:32.740838Z","shell.execute_reply.started":"2025-03-21T09:55:08.809025Z","shell.execute_reply":"2025-03-21T09:55:32.739722Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f1107899294299bde60877c835c148"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"def tag_is_inside_tile(bounds, x_start, y_start, width, height, truncated_percent):\n    x_min, y_min, x_max, y_max = bounds\n    x_min, y_min, x_max, y_max = x_min - x_start, y_min - y_start, x_max - x_start, y_max - y_start\n\n    if (x_min > width) or (x_max < 0.0) or (y_min > height) or (y_max < 0.0):\n        return None\n\n    x_max_trunc = min(x_max, width)\n    x_min_trunc = max(x_min, 0)\n    if (x_max_trunc - x_min_trunc) / (x_max - x_min) < truncated_percent:\n        return None\n\n    y_max_trunc = min(y_max, width)\n    y_min_trunc = max(y_min, 0)\n    if (y_max_trunc - y_min_trunc) / (y_max - y_min) < truncated_percent:\n        return None\n\n    x_center = (x_min_trunc + x_max_trunc) / 2.0 / width\n    y_center = (y_min_trunc + y_max_trunc) / 2.0 / height\n    x_extend = (x_max_trunc - x_min_trunc) / width\n    y_extend = (y_max_trunc - y_min_trunc) / height\n\n    return (0, x_center, y_center, x_extend, y_extend)\n\nfor img_path in tqdm.notebook.tqdm(img_list):\n    pil_img = PIL.Image.open(f\"{IMG_DIR}/{img_path}\", mode='r')\n    np_img = np.array(pil_img, dtype=np.uint8)\n\n    img_labels = df[df[\"img_name\"] == img_path]\n    X_TILES = (IMAGE_WIDTH + TILE_WIDTH - TILE_OVERLAP - 1) // (TILE_WIDTH - TILE_OVERLAP)\n    Y_TILES = (IMAGE_HEIGHT + TILE_HEIGHT - TILE_OVERLAP - 1) // (TILE_HEIGHT - TILE_OVERLAP)\n\n    for x in range(X_TILES):\n        for y in range(Y_TILES):\n\n            x_end = min((x + 1) * TILE_WIDTH - TILE_OVERLAP * (x != 0), IMAGE_WIDTH)\n            x_start = x_end - TILE_WIDTH\n            y_end = min((y + 1) * TILE_HEIGHT - TILE_OVERLAP * (y != 0), IMAGE_HEIGHT)\n            y_start = y_end - TILE_HEIGHT\n\n            folder = 'val' if img_path in val_index else 'train'\n            save_tile_path = TILES_DIR[folder].joinpath(img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + \".jpg\")\n            save_label_path = LABELS_DIR[folder].joinpath(img_path + \"_\" + str(x_start) + \"_\" + str(y_start) + \".txt\")\n\n            if _overwriteFiles or not os.path.isfile(save_tile_path):\n                cut_tile = np.zeros(shape=(TILE_WIDTH, TILE_HEIGHT, 3), dtype=np.uint8)\n                cut_tile[0:TILE_HEIGHT, 0:TILE_WIDTH, :] = np_img[y_start:y_end, x_start:x_end, :]\n                cut_tile_img = PIL.Image.fromarray(cut_tile)\n                cut_tile_img.save(save_tile_path)\n\n            found_tags = [\n                tag_is_inside_tile(bounds, x_start, y_start, TILE_WIDTH, TILE_HEIGHT, TRUNCATED_PERCENT)\n                for i, bounds in enumerate(img_labels['bounds'])]\n            found_tags = [el for el in found_tags if el is not None]\n\n            # save labels\n            with open(save_label_path, 'w+') as f:\n                for tags in found_tags:\n                    f.write(' '.join(str(x) for x in tags) + '\\n')","metadata":{"id":"ATBeCbqW-6kd","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f34a8bf789164884bcde9fd9aab95a76","7872c752444d4df486b882015ae67218","2b0ec1e1cde54a388e23d4f89c4d2c00","78b25f4e0c9543aaa6ea683452d2ef41","6a973618df3142adb0d3ce52437d85e0","ddb56ffce61442c292fb05ee47ea18d0","6c86344bc5254f11bacc112b587512f5","db7425f91cbe48c389fd5520954869c3","3e5e0e97a1c44a0da4c162eacff2469c","8c1d9348d0424738b118776b8471cd88","a7cdb06177504f478e41c83472a6bf6a"]},"outputId":"16a78365-d0a5-418a-dabb-ca11dc2f13ef","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:55:52.101134Z","iopub.execute_input":"2025-03-21T09:55:52.101487Z","iopub.status.idle":"2025-03-21T09:55:59.634071Z","shell.execute_reply.started":"2025-03-21T09:55:52.101461Z","shell.execute_reply":"2025-03-21T09:55:59.632844Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22cd1af8abfd42918ced1317686c2ed0"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# Yaml file for YOLO\n\nCONFIG = \"\"\"\n# train and val datasets (image directory or *.txt file with image paths)\ntrain: /kaggle/working/train3/\nval: /kaggle/working/val3/\n\n# number of classes\nnc: 2\n\n# class names\nnames: ['Signature','Logo']\n\"\"\"\n\nwith open(\"dataset.yaml\", \"w\") as f:\n    f.write(CONFIG)","metadata":{"id":"NGEO8CbICjpL","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:01:04.829054Z","iopub.execute_input":"2025-03-21T10:01:04.829345Z","iopub.status.idle":"2025-03-21T10:01:04.833873Z","shell.execute_reply.started":"2025-03-21T10:01:04.829323Z","shell.execute_reply":"2025-03-21T10:01:04.832938Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"!python /kaggle/working/yolov5/train.py --cfg yolov5s.yaml --imgsz 1200 --batch-size 16 --epochs 10 --data dataset.yaml --weights yolov5s.pt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Xlp1pGUEpOA","outputId":"c8dd8195-fd46-43ec-9796-86296d26ab9e","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:01:09.549191Z","iopub.execute_input":"2025-03-21T10:01:09.549472Z","iopub.status.idle":"2025-03-21T10:22:17.360694Z","shell.execute_reply.started":"2025-03-21T10:01:09.549450Z","shell.execute_reply":"2025-03-21T10:22:17.359773Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n2025-03-21 10:01:15.342881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-21 10:01:15.364471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-21 10:01:15.371830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=yolov5s.yaml, data=dataset.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=1200, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v7.0-399-g8cc44963 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 3.91MB/s]\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n100%|██████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 34.1MB/s]\n\nOverriding model.yaml nc=80 with nc=2\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nYOLOv5s summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n\nTransferred 342/349 items from yolov5s.pt\n/kaggle/working/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n/kaggle/working/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast(autocast):\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\nWARNING ⚠️ --img-size 1200 must be multiple of max stride 32, updating to 1216\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\nWARNING ⚠️ DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\nSee Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/train3/labels... 2550 images, 287 backgrounds, 0\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/train3/labels.cache\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/val3/labels... 280 images, 32 backgrounds, 0 corru\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/val3/labels.cache\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.17 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\nPlotting labels to yolov5/runs/train/exp3/labels.jpg... \n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/kaggle/working/yolov5/train.py:355: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=amp)\nImage sizes 1216 train, 1216 val\nUsing 2 dataloader workers\nLogging results to \u001b[1myolov5/runs/train/exp3\u001b[0m\nStarting training for 10 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n  0%|          | 0/160 [00:00<?, ?it/s]/kaggle/working/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n        0/9      6.31G    0.08008    0.08836      0.017         33       1216:  WARNING ⚠️ TensorBoard graph visualization failure Sizes of tensors must match except in dimension 1. Expected size 76 but got size 75 for tensor number 1 in the list.\n        0/9      6.31G    0.08008    0.08836      0.017         33       1216:  /kaggle/working/yolov5/train.py:412: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(amp):\n        0/9      6.31G    0.05807    0.03278   0.009493         12       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.156      0.417      0.152     0.0278\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        1/9      9.11G    0.03992    0.01584    0.00175          9       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372          1      0.419      0.656      0.253\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        2/9      9.12G    0.03531    0.01353   0.001187         15       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372       0.45       0.75      0.578      0.279\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        3/9      9.12G    0.03032    0.01236  0.0008548          8       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.998      0.667      0.861      0.505\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        4/9      9.12G    0.02839    0.01164  0.0006718         12       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.778      0.583      0.656      0.368\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        5/9      9.12G    0.02462    0.01045  0.0005029         15       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.908      0.833      0.877      0.583\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        6/9      9.12G     0.0247    0.01031  0.0004626         13       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.999      0.833       0.91        0.5\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        7/9      9.12G    0.02159    0.00972  0.0003096         13       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.998      0.917      0.953      0.614\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        8/9      9.12G    0.01875   0.009207  0.0002449          8       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.999      0.917      0.937      0.644\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        9/9      9.12G    0.01977   0.009003  0.0001944         10       1216: 1\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.998      0.917      0.947       0.67\n\n10 epochs completed in 0.333 hours.\nOptimizer stripped from yolov5/runs/train/exp3/weights/last.pt, 15.0MB\nOptimizer stripped from yolov5/runs/train/exp3/weights/best.pt, 15.0MB\n\nValidating yolov5/runs/train/exp3/weights/best.pt...\nFusing layers... \nYOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n                 Class     Images  Instances          P          R      mAP50   \n                   all        280        372      0.998      0.917      0.947       0.67\n             Signature        280        372      0.998      0.917      0.947       0.67\nResults saved to \u001b[1myolov5/runs/train/exp3\u001b[0m\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!python /kaggle/working/yolov5/detect.py --source /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img --img-size 4250 5500 --weights /kaggle/working/yolov5/runs/train/exp3/weights/best.pt --conf 0.5 --save-txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3Pt-tDUG8VW","outputId":"ef6fec03-4c80-4190-84f6-f6e60139ef9b","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:54:14.822686Z","iopub.execute_input":"2025-03-21T10:54:14.823109Z","iopub.status.idle":"2025-03-21T10:54:31.709599Z","shell.execute_reply.started":"2025-03-21T10:54:14.823073Z","shell.execute_reply":"2025-03-21T10:54:31.708440Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/kaggle/working/yolov5/runs/train/exp3/weights/best.pt'], source=/kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img, data=yolov5/data/coco128.yaml, imgsz=[4250, 5500], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v7.0-399-g8cc44963 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\nFusing layers... \nYOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\nWARNING ⚠️ --img-size [4250, 5500] must be multiple of max stride 32, updating to [4256, 5504]\nimage 1/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_0_pg2.jpg: 4256x3296 4 Signatures, 184.1ms\nimage 2/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_0_pg3.jpg: 4256x3296 1 Signature, 176.8ms\nimage 3/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_10_pg2.jpg: 4256x3296 3 Signatures, 176.2ms\nimage 4/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_10_pg3.jpg: 4256x3296 1 Signature, 176.4ms\nimage 5/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_1_pg2.jpg: 4256x3296 1 Signature, 175.9ms\nimage 6/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_1_pg3.jpg: 4256x3296 1 Signature, 175.4ms\nimage 7/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_2_pg2.jpg: 4256x3296 3 Signatures, 177.0ms\nimage 8/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_2_pg3.jpg: 4256x3296 1 Signature, 175.8ms\nimage 9/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_3_pg2.jpg: 4256x3296 4 Signatures, 176.9ms\nimage 10/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_3_pg3.jpg: 4256x3296 1 Signature, 175.1ms\nimage 11/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_4_pg2.jpg: 4256x3296 4 Signatures, 176.1ms\nimage 12/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_4_pg3.jpg: 4256x3296 1 Signature, 176.6ms\nimage 13/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_5_pg2.jpg: 4256x3296 3 Signatures, 179.0ms\nimage 14/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_5_pg3.jpg: 4256x3296 1 Signature, 175.4ms\nimage 15/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_7_pg2.jpg: 4256x3296 2 Signatures, 179.5ms\nimage 16/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_7_pg3.jpg: 4256x3296 1 Signature, 176.8ms\nimage 17/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_8_pg2.jpg: 4256x3296 4 Signatures, 179.5ms\nimage 18/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_8_pg3.jpg: 4256x3296 1 Signature, 176.2ms\nimage 19/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_9_pg2.jpg: 4256x3296 3 Signatures, 177.1ms\nimage 20/20 /kaggle/input/signaturelogo-images-annotation-v2/SignatureLogo_images_annotation/img/Detailed_Divorce_Agreement_9_pg3.jpg: 4256x3296 1 Signature, 177.0ms\nSpeed: 11.9ms pre-process, 177.1ms inference, 10.3ms NMS per image at shape (1, 3, 4256, 5504)\nResults saved to \u001b[1myolov5/runs/detect/exp7\u001b[0m\n20 labels saved to yolov5/runs/detect/exp7/labels\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# import glob\n# from IPython.display import Image, display\n\n# for image_path in glob.glob('yolov5/runs/detect/exp/*.jpg'):\n#       display(Image(filename=image_path, width=1024))\n#       print(\"\\n\")","metadata":{"id":"lQZ0sYHxH4x8","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:25:24.038471Z","iopub.execute_input":"2025-03-21T10:25:24.038842Z","iopub.status.idle":"2025-03-21T10:25:24.042467Z","shell.execute_reply.started":"2025-03-21T10:25:24.038813Z","shell.execute_reply":"2025-03-21T10:25:24.041527Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Gliner","metadata":{"id":"PSWmFM6fIRHn"}},{"cell_type":"code","source":"!pip install gliner\n!pip install PyMuPDF\n!pip install fpdf\n!pip install pdf2image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:26:21.032362Z","iopub.execute_input":"2025-03-21T10:26:21.032707Z","iopub.status.idle":"2025-03-21T10:26:40.597160Z","shell.execute_reply.started":"2025-03-21T10:26:21.032685Z","shell.execute_reply":"2025-03-21T10:26:40.596262Z"}},"outputs":[{"name":"stdout","text":"Collecting gliner\n  Downloading gliner-0.2.17-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gliner) (2.5.1+cu121)\nRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from gliner) (4.47.0)\nRequirement already satisfied: huggingface_hub>=0.21.4 in /usr/local/lib/python3.10/dist-packages (from gliner) (0.29.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gliner) (4.67.1)\nCollecting onnxruntime (from gliner)\n  Downloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from gliner) (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.4->gliner) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->gliner) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->gliner) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->gliner) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->gliner) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->gliner) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->gliner) (0.4.5)\nCollecting coloredlogs (from onnxruntime->gliner)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->gliner) (24.3.25)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->gliner) (3.20.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.38.2->gliner) (2.4.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->gliner) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->gliner) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->gliner) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.38.2->gliner) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.38.2->gliner) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers>=4.38.2->gliner) (2024.2.0)\nDownloading gliner-0.2.17-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, gliner\nSuccessfully installed coloredlogs-15.0.1 gliner-0.2.17 humanfriendly-10.0 onnxruntime-1.21.0\nCollecting PyMuPDF\n  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.25.4\nCollecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=0aa932764e853938e05d4b0c729ad1209d358e31756933cfbd6f025187d3c613\n  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\nSuccessfully built fpdf\nInstalling collected packages: fpdf\nSuccessfully installed fpdf-1.7.2\nRequirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import fitz  # PyMuPDF\nfrom gliner import GLiNER\n\n# --- Initialize GLiNER ---\n#model = GLiNER.from_pretrained(\"urchade/gliner_large-v2.1\")\nmodel = GLiNER.from_pretrained(\"urchade/gliner_multi_pii-v1\")\nlabels = [\"Person\", \"Money\", \"Date\", \"Tenure\", \"Country\", \"Street_Address\", \"State\", \"City\"]\n\ndef replace_text_with_labels(input_path, output_path):\n    doc = fitz.open(input_path)\n    fontname = \"Helvetica\"\n\n    for page_num in range(len(doc)):\n        page = doc[page_num]\n        full_text = page.get_text()\n\n        # Predict entities and filter overlaps\n        entities = model.predict_entities(full_text, labels=labels, threshold=0.5)\n        entities = resolve_overlaps_and_errors(entities)  # <-- New overlap resolution\n\n        # Sort entities by position (reverse order to avoid coordinate shifts)\n        entities.sort(key=lambda x: x[\"start\"], reverse=True)\n\n        # Replace entities in the PDF\n        for entity in entities:\n            original_text = entity[\"text\"]\n            label = entity[\"label\"]\n\n            # Find all occurrences of the entity on the page\n            text_instances = page.search_for(original_text)\n\n            for inst in text_instances:\n                # Get original text properties\n                block = find_text_block(page.get_text(\"dict\"), inst)\n                if not block:\n                    continue\n\n                # Replace text with label using standard font\n                page.add_redact_annot(\n                    inst,\n                    text=f\"[{label}]\",\n                    fontsize=block[\"size\"],\n                    fontname=fontname,\n                    text_color=block[\"color\"],\n                    fill=(1, 1, 1)\n                )\n\n        # Apply all redactions on the page\n        page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_NONE)\n\n    doc.save(output_path, garbage=4, deflate=True, incremental=False)\n    doc.close()\n    print(f\"✅ Modified PDF saved to: {output_path}\")\n\ndef resolve_overlaps_and_errors(entities):\n    \"\"\"Resolve overlaps AND filter incorrect labels like 'marriage' → [State]\"\"\"\n    priority_labels = [\"Street_Address\", \"State\", \"City\", \"Country\"]\n    resolved = []\n\n    # Sort by length (longest first) to prioritize broader matches\n    entities.sort(key=lambda x: x[\"end\"] - x[\"start\"], reverse=True)\n\n    for entity in entities:\n        text = entity[\"text\"].lower()  # Case-insensitive check\n\n        # --- Custom Rule to Fix \"marriage\" → [State] ---\n        if entity[\"label\"] == \"State\" and \"Address\" in text:\n            continue  # Skip this entity\n\n        # --- Custom Rule to Fix \"ia\" → [State] ---\n        if entity[\"label\"] == \"State\" and text == \"ia\":\n            continue  # Skip standalone \"ia\" (if not part of a valid context)\n\n        # --- Standard Overlap Resolution ---\n        is_contained = False\n        for resolved_entity in resolved:\n            if (entity[\"start\"] >= resolved_entity[\"start\"] and\n                entity[\"end\"] <= resolved_entity[\"end\"]):\n                is_contained = True\n                break\n\n        if not is_contained:\n            resolved.append(entity)\n\n    return resolved\n\ndef find_text_block(text_dict, rect):\n    \"\"\"Find text block containing the given rectangle\"\"\"\n    for block in text_dict[\"blocks\"]:\n        if \"lines\" not in block:\n            continue\n        for line in block[\"lines\"]:\n            for span in line[\"spans\"]:\n                span_rect = fitz.Rect(span[\"bbox\"])\n                if span_rect.intersects(rect):\n                    return span\n    return None\n\n# --- Usage ---\ninput_pdf = \"/kaggle/input/divorce-pdf/Detailed_Divorce_Agreement_8.pdf\"\noutput_pdf = \"Modified_Divorce_Agreement_8.pdf\"\nreplace_text_with_labels(input_pdf, output_pdf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:29:52.649776Z","iopub.execute_input":"2025-03-21T10:29:52.650210Z","iopub.status.idle":"2025-03-21T10:30:50.350635Z","shell.execute_reply.started":"2025-03-21T10:29:52.650179Z","shell.execute_reply":"2025-03-21T10:30:50.349747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9512efd42da048df9a8093ab8f119d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5010d9da08e34ea9ba9e1721a8f8f481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f509f275615d4a99b315269fd994be9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gliner_config.json:   0%|          | 0.00/478 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74611018d234bd3bec9412d3eb97824"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0620c8dc9947838e109ecb2d2c8025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f51589fad42a469492a8a6e4cfb9d800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d70952188c4291ab59c035df11a507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c8661b91204bc697c6a9666f4ab1ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 464 has been truncated to 384\n  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/usr/local/lib/python3.10/dist-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 449 has been truncated to 384\n  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n","output_type":"stream"},{"name":"stdout","text":"✅ Modified PDF saved to: Modified_Divorce_Agreement_8.pdf\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"!apt-get install poppler-utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:33:55.213505Z","iopub.execute_input":"2025-03-21T10:33:55.214283Z","iopub.status.idle":"2025-03-21T10:34:07.386783Z","shell.execute_reply.started":"2025-03-21T10:33:55.214249Z","shell.execute_reply":"2025-03-21T10:34:07.385874Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 126 not upgraded.\nNeed to get 1,462 kB of archives.\nAfter this operation, 696 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.6 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.6 [5,184 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.6 [1,071 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\nFetched 1,462 kB in 3s (427 kB/s)      \n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.4) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"os.makedirs(\"images_gliner\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:38:19.733649Z","iopub.execute_input":"2025-03-21T10:38:19.733977Z","iopub.status.idle":"2025-03-21T10:38:19.738435Z","shell.execute_reply.started":"2025-03-21T10:38:19.733952Z","shell.execute_reply":"2025-03-21T10:38:19.737667Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from pdf2image import convert_from_path\nfrom PIL import Image\npdf_list = ['/kaggle/working/Modified_Divorce_Agreement_8.pdf']\nfor pdfs in pdf_list:\n    file_name_image = pdfs.split(r\"/\")[-1].split(\".\")[0]\n    pages = convert_from_path(pdfs, 500)\n\n    for count, page in enumerate(pages):\n\n        page.save(rf'images_gliner/{file_name_image}_pg{count}.jpg', 'JPEG')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:38:45.883232Z","iopub.execute_input":"2025-03-21T10:38:45.883529Z","iopub.status.idle":"2025-03-21T10:38:47.976818Z","shell.execute_reply.started":"2025-03-21T10:38:45.883506Z","shell.execute_reply":"2025-03-21T10:38:47.976092Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"!python /kaggle/working/yolov5/detect.py --source /kaggle/working/images_gliner --img-size 4250 5500 --weights /kaggle/working/yolov5/runs/train/exp3/weights/best.pt --conf 0.5 --save-txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T10:54:39.391245Z","iopub.execute_input":"2025-03-21T10:54:39.391569Z","iopub.status.idle":"2025-03-21T10:54:48.266210Z","shell.execute_reply.started":"2025-03-21T10:54:39.391542Z","shell.execute_reply":"2025-03-21T10:54:48.265302Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/kaggle/working/yolov5/runs/train/exp3/weights/best.pt'], source=/kaggle/working/images_gliner, data=yolov5/data/coco128.yaml, imgsz=[4250, 5500], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v7.0-399-g8cc44963 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n\nFusing layers... \nYOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\nWARNING ⚠️ --img-size [4250, 5500] must be multiple of max stride 32, updating to [4256, 5504]\nimage 1/4 /kaggle/working/images_gliner/Modified_Divorce_Agreement_8_pg0.jpg: 4256x3296 (no detections), 187.5ms\nimage 2/4 /kaggle/working/images_gliner/Modified_Divorce_Agreement_8_pg1.jpg: 4256x3296 (no detections), 181.1ms\nimage 3/4 /kaggle/working/images_gliner/Modified_Divorce_Agreement_8_pg2.jpg: 4256x3296 2 Signatures, 177.1ms\nimage 4/4 /kaggle/working/images_gliner/Modified_Divorce_Agreement_8_pg3.jpg: 4256x3296 1 Signature, 177.8ms\nSpeed: 11.7ms pre-process, 180.9ms inference, 39.7ms NMS per image at shape (1, 3, 4256, 5504)\nResults saved to \u001b[1myolov5/runs/detect/exp8\u001b[0m\n2 labels saved to yolov5/runs/detect/exp8/labels\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import cv2\nimport os\n\n# Paths\nsource_dir = \"/kaggle/working/yolov5/runs/detect/exp8\"  # Directory containing detected images\nlabels_dir = \"/kaggle/working/yolov5/runs/detect/exp8/labels\"  # Directory containing detection .txt files\noutput_dir = \"/kaggle/working/yolov5/runs/detect/exp8_masked\"  # Directory to save modified images\n\n\nos.makedirs(output_dir, exist_ok=True)\n\nfor image_name in os.listdir(source_dir):\n    if not image_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Skip non-image files\n        continue\n    \n    image_path = os.path.join(source_dir, image_name)\n    label_path = os.path.join(labels_dir, os.path.splitext(image_name)[0] + \".txt\")\n\n    img = cv2.imread(image_path)\n    h, w = img.shape[:2]\n    \n    if os.path.exists(label_path):\n        with open(label_path, \"r\") as f:\n            lines = f.readlines()\n        \n        # Draw black rectangles for each detection\n        for line in lines:\n            parts = line.strip().split()\n            \n       \n            x_center, y_center, bbox_width, bbox_height = map(float, parts[1:5])\n            x1 = int((x_center - bbox_width / 2) * w)\n            y1 = int((y_center - bbox_height / 2) * h)\n            x2 = int((x_center + bbox_width / 2) * w)\n            y2 = int((y_center + bbox_height / 2) * h)\n            \n       \n            label_height_extension = int(bbox_height * 0.5)  \n            y1_extended = max(0, y1 - label_height_extension)  \n            \n            \n            cv2.rectangle(img, (x1, y1_extended), (x2, y2), (0, 0, 0), thickness=-1)  # thickness=-1 fills the rectangle\n\n\n    output_path = os.path.join(output_dir, image_name)\n    cv2.imwrite(output_path, img)\n\nprint(\"Black masks applied and modified images saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T11:01:27.434093Z","iopub.execute_input":"2025-03-21T11:01:27.434409Z","iopub.status.idle":"2025-03-21T11:01:28.275818Z","shell.execute_reply.started":"2025-03-21T11:01:27.434385Z","shell.execute_reply":"2025-03-21T11:01:28.275053Z"}},"outputs":[{"name":"stdout","text":"Black masks applied and modified images saved successfully!\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"from PIL import Image\n\nimage_paths = [f\"/kaggle/working/yolov5/runs/detect/exp8_masked/Modified_Divorce_Agreement_8_pg{i}.jpg\" for i in range(4)]  # Adjust the range based on the number of images\n\n\nfirst_image = Image.open(image_paths[0])\n\nother_images = [Image.open(img_path).convert(\"RGB\") for img_path in image_paths[1:]]\n\n\noutput_pdf_path = \"Detailed_Divorce_Agreement_8_redacted.pdf\"\nfirst_image.convert(\"RGB\").save(output_pdf_path, save_all=True, append_images=other_images)\n\nprint(f\"Combined PDF saved at: {output_pdf_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T11:01:29.255333Z","iopub.execute_input":"2025-03-21T11:01:29.255613Z","iopub.status.idle":"2025-03-21T11:01:30.252482Z","shell.execute_reply.started":"2025-03-21T11:01:29.255593Z","shell.execute_reply":"2025-03-21T11:01:30.251650Z"}},"outputs":[{"name":"stdout","text":"Combined PDF saved at: Detailed_Divorce_Agreement_8_redacted.pdf\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}